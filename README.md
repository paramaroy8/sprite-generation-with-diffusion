## Project Description

In this project, image-to-image generation has been implemented using diffusion model. The U-net architecture with context and time embeddings was used as the diffusion model. 16-by-16 Sprite images were used to train and evaluate the model. 

## Why Sprite Images Instead Of Real-life Images?

Sprite images have reduced complexities compared to real-life images. As a result, this dataset is less computationally expensive, making it a efficient for fast experimentation with different types of models and techniques.

## Acknowledgments

Dataset: [Sprite Images Dataset](https://huggingface.co/datasets/ashis-palai/sprites_image_dataset/tree/main)

Papers: [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239), [Denoising Diffusion Implicit Models](https://arxiv.org/abs/2010.02502)

Tutorial: [How Diffusion Models Work](https://learn.deeplearning.ai/courses/diffusion-models/lesson/xb8aa/introduction)

GitHub Repo: [minDiffusion](https://github.com/cloneofsimo/minDiffusion)
